#!/usr/bin/env python3

import subprocess
import os
import sys
import argparse
import platform
from huggingface_hub import snapshot_download, create_repo, HfApi

def sh(command):
    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, text=True, universal_newlines=True)
    if process.stdout is None:
        sys.stderr.write(f"Error: failed to capture stdout for {command}\n")
    else:
        try:
            for line in process.stdout:
                if args.verbose:
                    print(line, end='')
        except Exception as e:
            sys.stderr.write(f"Error while reading output: {e}\n")

    process.wait()

    if process.returncode != 0:
        sys.stderr.write(f"Error: failed with return code {process.returncode}\n")
        exit(1)

def update_llama_cpp():
    if args.no_accelerator:
        accelerator = ''
        if args.verbose:
            print("üê™ llama.cpp will be compiled without GPU acceleration.")
    else:
        accelerator = 'LLAMA_METAL=1' if platform.system() == 'Darwin' else 'LLAMA_CUBLAS=1'
        if args.verbose:
            accelerator_string = "Metal" if platform.system() == 'Darwin' else "CUDA"
            print(f"üê™ llama.cpp will be compiled with {accelerator_string} acceleration.")

    if not os.path.exists(f"{LLAMA_PATH}"):
        if args.verbose:
            print(f"üê™ llama.cpp not found at {LLAMA_PATH}, installing...")
        sh(f"git clone https://github.com/ggerganov/llama.cpp {LLAMA_PATH}")
    if args.verbose:
        print("üê™ compiling llama.cpp...")
    sh(f"cd {LLAMA_PATH} && git pull && make clean && {accelerator} make")
    if args.verbose:
        print("üê™ installing llama.cpp python deps...")
    sh(f"pip3 install -r {LLAMA_PATH}/requirements.txt")

def download_model():
    sh(f"mkdir {MODEL_NAME}")
    if args.verbose:
        print(f"ü§ó downloading {MODEL_NAME}...")
    snapshot_download(repo_id=MODEL_ID, local_dir=f"./{MODEL_NAME}/", local_dir_use_symlinks=False)

def convert_fp16(type="llama", vocab_type=None):
    if args.verbose:
        print(f"ü™Ñ converting {MODEL_NAME} to FP16...")
    if type == "llama" or type == "mistral":
        pad_vocab = " --pad-vocab" if args.pad_vocab else ""
        vocab_type = f" --vocab-type {vocab_type}" if vocab_type != None else ""
        sh(f"python3 {LLAMA_PATH}/convert.py {MODEL_NAME} --outtype f16 --outfile {FP16}{vocab_type}{pad_vocab}")
    else:
        sh(f"python3 {LLAMA_PATH}/convert-hf-to-gguf.py {MODEL_NAME} --outtype f16 --outfile {FP16}")

# build importance matrix with FP16 model & a calibration dataset; required for `IQ2_*` or `Q2_K_S` quants
def generate_imatrix():
    if args.verbose:
        print(f"üåê downloading wikitext-2...")
    sh(f"{LLAMA_PATH}/scripts/get-wikitext-2.sh && unzip wikitext-2-raw-v1.zip && rm wikitext-2-raw-v1.zip")
    if args.verbose:
        print(f"‚öñÔ∏è generating imatrix for {MODEL_NAME}...")
    sh(f"{LLAMA_PATH}/imatrix -m {FP16} -f wikitext-2-raw/wiki.train.raw -o {MODEL_NAME}.imatrix -t 7 -ngl 999 --chunks 2000")
    if args.verbose:
        print(f"üßπ cleaning up wikitext...")
    sh(f"rm -rf wikitext-2-raw")

# ü§ó Upload to HuggingFace Hub
def upload_ggufs_to_hf():
    if args.verbose:
        print(f"ü§ó uploading {MODEL_NAME} to HuggingFace Hub...")
    token = args.hf_token or os.environ.get('HF_TOKEN')
    api = HfApi(token=token)
    repo_id = f"{args.hf_user}/{MODEL_NAME}-GGUF"

    create_repo(
        repo_id = repo_id,
        repo_type="model",
        exist_ok=True,
        token=token
    )

    # TODO: Generate a README from a template!

    # Upload only gguf files
    api.upload_folder(
        folder_path=MODEL_NAME,
        repo_id=repo_id,
        allow_patterns=f"*.gguf" # allow_patterns=f"*.gguf,README.md"
    )

NORMAL_QUANT_LEVELS = ["q2_k","q3_k_s","q3_k_m","q3_k_l","q4_0","q4_1","q4_k_s","q4_k_m","q5_0","q5_1","q5_k_s","q5_k_m","q6_k","q8_0"]
IMATRIX_QUANT_LEVELS = ["iq2_xxs","iq2_xs","q2_k_s"]

parser = argparse.ArgumentParser(description='convert HuggingFace models to GGUF, automatically.')

# REQUIRED
parser.add_argument('model_id', type=str, help='The HuggingFace model ID to convert. Required.')

# OPTIONAL
parser.add_argument('--quants', '-q', type=str, default='q2_k,q3_k_s,q3_k_m,q3_k_l,q4_k_s,q4_k_m,q5_k_s,q5_k_m,q6_k,q8_0', help='Comma-separated list of quant levels to convert. Defaults to all non-imatrix k-quants and 8_0.\nAll quantization levels: `q2_k`, `q2_k_s` `q3_k_l`, `q3_k_m`, `q3_k_s`, `q4_0`, `q4_1`, `q4_k_m`, `q4_k_s`, `q5_0`, `q5_1`, `q5_k_m`, `q5_k_s`, `q6_k`, `q8_0`, `iq2_xxs`, `iq2_xs`.')

parser.add_argument('--verbose', '-v', action='store_true', help='Increase output verbosity.')
parser.add_argument('--model-type', '-m', type=str, default='llama', help='The model architecture type. `llama` (default), `mistral`, or anything else.')
parser.add_argument('--vocab-type', '-vt', type=str, default=None, help='The model vocabulary type: `spm`, `hfft`, `bpe`, or None (default). Ignored for model-type other than `llama` or `mistral`.')
parser.add_argument('--pad-vocab', '-pv', action='store_true', help='Pad the vocabulary in case of mismatches. Defaults to false.')
parser.add_argument('--fp16', type=str, help='Path to fp16 GGUF file for quantization. Implies skipping conversion of to FP16.')
parser.add_argument('--skip-download', action='store_true', help='Skip downloading the model to convert from HuggingFace Hub. Defaults to false.')
parser.add_argument('--skip-upload', action='store_true', help='Skip uploading converted files to HuggingFace Hub. Defaults to false.')
parser.add_argument('--only-upload', action='store_true', help='Upload the .gguf files in the current directory to HuggingFace Hub. Defaults to false.')
parser.add_argument('--no-accelerator', '-n', action='store_true', help='Disable GPU acceleration for llama.cpp compilation. Only takes effect when installing or updating llama.cpp. Defaults to false.')
parser.add_argument('--update-llama', '-u', action='store_true', help='Update the llama.cpp repo before converting. Installs llama.cpp if llama-path doesn‚Äôt exist. Defaults to false.')
parser.add_argument('--llama-path', '-lp',  type=str, default='~/code/llama.cpp', help='The path to the llama.cpp repo.')
parser.add_argument('--hf-user', type=str, default='brittlewis12', help='Your HuggingFace username for uploading converted models.')
parser.add_argument('--hf-token', type=str, default=None, help='Your HuggingFace API token for uploading converted models. Reads from `$HF_TOKEN` by default.')

args = parser.parse_args()
if args.verbose:
    print(f"Got args: {args}\n")

LLAMA_PATH = os.path.expanduser(args.llama_path)
MODEL_ID = args.model_id # e.g. stabilityai/stablelm-2-zephyr-1_6b
MODEL_NAME = MODEL_ID.split('/')[-1]
FP16 = args.fp16 or os.path.join(MODEL_NAME, f"{MODEL_NAME.lower()}.fp16.gguf")
requested_quants = args.quants.replace(' ', '').split(',')

quants = [ql for ql in requested_quants if ql in NORMAL_QUANT_LEVELS]
imatrix_quants = [ql for ql in requested_quants if ql in IMATRIX_QUANT_LEVELS]

if imatrix_quants != []:
    print(f"ü™Ñ Quantizing {MODEL_NAME} with {', '.join(quants)} and imatrix-required {', '.join(imatrix_quants)}\n")
else:
    print(f"ü™Ñ Quantizing {MODEL_NAME} with {', '.join(quants)}\n")

if args.update_llama:
    update_llama_cpp()

if args.verbose:
    sh('pwd')

if args.skip_download or args.only_upload:
    print('ü§ó skipping download from HuggingFace Hub.')
else:
    download_model()
    if args.verbose:
        print(f"ü§ó downloaded {MODEL_NAME}!")

if args.fp16 or args.only_upload:
    print('skipping FP16 conversion.')
else:
    convert_fp16(type=args.model_type, vocab_type=args.vocab_type)

if imatrix_quants != [] and not args.only_upload:
    generate_imatrix()

# Qu-Qu-Qu-Quantize Time!
# TODO: can this be paralellized...?
# TODO: progress bar!
if not args.only_upload:
    for level in quants:
        if args.verbose:
            print(f"ü™Ñ quantizing {MODEL_NAME} to {level}...")
        qtype = f"{MODEL_NAME}/{MODEL_NAME.lower()}.{level.upper()}.gguf"
        sh(f"{LLAMA_PATH}/quantize {FP16} {qtype} {level}")

    for level in imatrix_quants:
        if args.verbose:
            print(f"ü™Ñ quantizing {MODEL_NAME} to {level}...")
        qtype = f"{MODEL_NAME}/{MODEL_NAME.lower()}.{level.upper()}.gguf"
        sh(f"{LLAMA_PATH}/quantize --imatrix {MODEL_NAME}.imatrix {FP16} {qtype} {level}")

if args.skip_upload:
    print("ü§ó skipping upload to HuggingFace Hub.")
else:
    upload_ggufs_to_hf()

print("üéâ done!")
