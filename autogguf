#!/usr/bin/env python3

import subprocess
import os
import sys
import argparse
import platform
from huggingface_hub import snapshot_download, create_repo, HfApi

def sh(command):
    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, text=True, universal_newlines=True)
    if process.stdout is None:
        sys.stderr.write(f"Error: failed to capture stdout for {command}\n")
    else:
        try:
            for line in process.stdout:
                if args.verbose:
                    print(line, end='')
        except Exception as e:
            sys.stderr.write(f"Error while reading output: {e}\n")

    process.wait()

    if process.returncode != 0:
        sys.stderr.write(f"Error: failed with return code {process.returncode}\n")
        exit(1)

def update_llama_cpp():
    if args.no_accelerator:
        accelerator = ''
        if args.verbose:
            print("üê™ llama.cpp will be compiled without GPU acceleration.")
    else:
        accelerator = 'LLAMA_METAL=1' if platform.system() == 'Darwin' else 'LLAMA_CUBLAS=1'
        if args.verbose:
            accelerator_string = "Metal" if platform.system() == 'Darwin' else "CUDA"
            print(f"üê™ llama.cpp will be compiled with {accelerator_string} acceleration.")

    if not os.path.exists(f"{LLAMA_PATH}"):
        if args.verbose:
            print(f"üê™ llama.cpp not found at {LLAMA_PATH}, installing...")
        sh(f"git clone https://github.com/ggerganov/llama.cpp {LLAMA_PATH}")
    if args.verbose:
        print("üê™ compiling llama.cpp...")
    sh(f"cd {LLAMA_PATH} && git pull && make clean && {accelerator} make")
    if args.verbose:
        print("üê™ installing llama.cpp python deps...")
    sh(f"pip3 install -r {LLAMA_PATH}/requirements.txt")

def download_model():
    sh(f"mkdir {MODEL_NAME}")
    if args.verbose:
        print(f"ü§ó downloading {MODEL_NAME}...")
    snapshot_download(repo_id=MODEL_ID, local_dir=f"./{MODEL_NAME}/")

def convert_fp():
    if args.verbose:
        print(f"ü™Ñ converting {MODEL_NAME} to {precision_label.upper()}...")
    sh(f"python3 {LLAMA_PATH}/convert_hf_to_gguf.py {MODEL_NAME} --outtype {PRECISION} --outfile {FP}")

# build importance matrix with FP model & a calibration dataset; required for `IQ2_*` or `Q2_K_S` quants
def generate_imatrix():
    if args.verbose:
        print(f"üåê downloading calibration dataset...")
    sh(f"wget https://github.com/ggerganov/llama.cpp/files/14194570/groups_merged.txt")
    if args.verbose:
        print(f"‚öñÔ∏è generating imatrix for {MODEL_NAME}...")
    sh(f"{LLAMA_PATH}/llama-imatrix -m {FP} -f groups_merged.txt -o {MODEL_NAME}/{MODEL_NAME}.imatrix -t 7 -ngl 999 --chunks 2000")
    if args.verbose:
        print(f"üßπ cleaning up caliration dataset...")
    sh(f"rm groups_merged.txt")

# ü§ó Upload to HuggingFace Hub
def upload_ggufs_to_hf():
    if args.verbose:
        print(f"ü§ó uploading {MODEL_NAME} to HuggingFace Hub...")
    token = args.hf_token or os.environ.get('HF_TOKEN')
    api = HfApi(token=token)
    repo_id = f"{args.hf_user}/{MODEL_NAME}-GGUF"

    create_repo(
        repo_id = repo_id,
        repo_type="model",
        exist_ok=True,
        token=token
    )

    # TODO: Generate a README from a template!

    # Upload only gguf files
    fut = api.upload_folder(
        folder_path=MODEL_NAME,
        repo_id=repo_id,
        allow_patterns=[f"*.gguf","*.imatrix"],
        run_as_future=True,
    )
    if args.verbose:
        fut.add_done_callback(lambda fut: print(f"ü§ó uploaded {MODEL_NAME} to HuggingFace Hub!\n    commit info: {fut.result()}"))
    return fut

FULL_PRECISION_QUANT_LEVELS = ["f16","f32"]
NORMAL_QUANT_LEVELS = ["q2_k","q3_k_s","q3_k_m","q3_k_l","q4_0","q4_1","q4_k_s","q4_k_m","q5_0","q5_1","q5_k_s","q5_k_m","q6_k","q8_0", "bf16"]
IMATRIX_QUANT_LEVELS = ["iq1_s","iq1_m","iq2_xxs","iq2_xs","iq2_s","iq2_m","q2_k_s","iq3_xxs","iq3_xs","iq3_s","iq3_m","iq4_xs","iq4_nl"]
ALL_QUANT_LEVELS_STR = ",".join([f"`{q}`" for q in NORMAL_QUANT_LEVELS + IMATRIX_QUANT_LEVELS])

parser = argparse.ArgumentParser(description='convert HuggingFace models to GGUF, automatically.')

# REQUIRED
parser.add_argument('model_id', type=str, help='The HuggingFace model ID to convert. Required.')

# OPTIONAL
parser.add_argument('--quants', '-q', type=str, default='q2_k,q3_k_s,q3_k_m,q3_k_l,q4_k_s,q4_k_m,q5_k_s,q5_k_m,q6_k,q8_0', help="Comma-separated list of quant levels to convert. Defaults to all non-imatrix k-quants and 8_0. All quantization levels: " + ALL_QUANT_LEVELS_STR)
parser.add_argument('--verbose', '-v', action='store_true', help='Increase output verbosity.')
parser.add_argument('--full-precision', type=str, default='f16', help='The full precision GGUF format to convert to and quantize from. `f16` (default) or `f32`.')
parser.add_argument('--fp', type=str, help='Path to fp16 or fp32 GGUF file for quantization. Implies skipping download and initial conversion to FP16.')
parser.add_argument('--imatrix', type=str, help='Path to custom imatrix file for imatrix quantization. Skips downloading calibration dataset and generating imatrix.')
parser.add_argument('--skip-download', action='store_true', help='Skip downloading the model to convert from HuggingFace Hub. Defaults to false.')
parser.add_argument('--skip-upload', action='store_true', help='Skip uploading converted files to HuggingFace Hub. Defaults to false.')
parser.add_argument('--only-upload', action='store_true', help='Upload the .gguf files in the current directory to HuggingFace Hub. Defaults to false.')
parser.add_argument('--no-accelerator', '-n', action='store_true', help='Disable GPU acceleration for llama.cpp compilation. Only takes effect when installing or updating llama.cpp. Defaults to false.')
parser.add_argument('--update-llama', '-u', action='store_true', help='Update the llama.cpp repo before converting. Installs llama.cpp if llama-path doesn‚Äôt exist. Defaults to false.')
parser.add_argument('--llama-path', '-lp',  type=str, default='~/code/llama.cpp', help='The path to the llama.cpp repo.')
parser.add_argument('--hf-user', type=str, default='brittlewis12', help='Your HuggingFace username for uploading converted models.')
parser.add_argument('--hf-token', type=str, default=None, help='Your HuggingFace API token for uploading converted models. Reads from `$HF_TOKEN` by default.')

args = parser.parse_args()
if args.verbose:
    print(f"Got args: {args}\n")

LLAMA_PATH = os.path.expanduser(args.llama_path)
MODEL_ID = args.model_id # e.g. stabilityai/stablelm-2-zephyr-1_6b
MODEL_NAME = MODEL_ID.split('/')[-1]
PRECISION = 'f16'
if args.full_precision.lower() == 'f32':
    PRECISION = 'f32'
else:
    if args.verbose and args.full_precision.lower() not in FULL_PRECISION_QUANT_LEVELS:
        print(f"Invalid full precision format '{args.full_precision}'. Defaulting to f16.")
    PRECISION = 'f16'

precision_label = 'fp16' if PRECISION == 'f16' else 'fp32'

FP = args.fp or os.path.join(MODEL_NAME, f"{MODEL_NAME.lower()}.{precision_label}.gguf")
requested_quants = args.quants.replace(' ', '').split(',')

quants = [ql for ql in requested_quants if ql in NORMAL_QUANT_LEVELS]
imatrix_quants = [ql for ql in requested_quants if ql in IMATRIX_QUANT_LEVELS]

if imatrix_quants != []:
    print(f"ü™Ñ Quantizing {MODEL_NAME} with {', '.join(quants)} and imatrix-required {', '.join(imatrix_quants)}\n")
else:
    print(f"ü™Ñ Quantizing {MODEL_NAME} with {', '.join(quants)}\n")

if args.update_llama:
    update_llama_cpp()

if args.verbose:
    sh('pwd')

if args.skip_download or args.only_upload or args.fp:
    print('ü§ó skipping download from HuggingFace Hub.')
else:
    download_model()
    if args.verbose:
        print(f"ü§ó downloaded {MODEL_NAME}!")

if args.fp or args.only_upload:
    print(f"skipping {precision_label.upper()} conversion.")
else:
    convert_fp()

if imatrix_quants != [] and not args.only_upload and not args.imatrix:
    generate_imatrix()

internal = {
    'upload_futs': [],
    'needs_upload': False,
    'uploading': False,
}
# Qu-Qu-Qu-Quantize Time!
# TODO: can this be paralellized...?
# TODO: progress bar!
if not args.only_upload:
    for level in quants:
        if args.verbose:
            print(f"ü™Ñ quantizing {MODEL_NAME} to {level}...")
        qtype = f"{MODEL_NAME}/{MODEL_NAME.lower()}.{level.upper()}.gguf"
        sh(f"{LLAMA_PATH}/llama-quantize {FP} {qtype}.pending {level}")
        sh(f"mv {qtype}.pending {qtype}")
        if args.skip_upload:
            if args.verbose:
                print("ü§ó skipping upload to HuggingFace Hub.")
        else:
            if internal['uploading']:
                internal['needs_upload'] = True
            else:
                internal['uploading'] = True
                internal['needs_upload'] = False
                fut = upload_ggufs_to_hf()
                fut.add_done_callback(lambda f: internal.__setitem__('uploading', False))
                internal['upload_futs'].append(fut)

    for level in imatrix_quants:
        imatrix = args.imatrix or f"{MODEL_NAME}/{MODEL_NAME}.imatrix"
        if args.verbose:
            print(f"ü™Ñ quantizing {MODEL_NAME} to {level} with {imatrix}...")
        qtype = f"{MODEL_NAME}/{MODEL_NAME.lower()}.{level.upper()}.gguf"
        sh(f"{LLAMA_PATH}/llama-quantize --imatrix {imatrix} {FP} {qtype}.pending {level}")
        sh(f"mv {qtype}.pending {qtype}")
        if args.skip_upload:
            if args.verbose:
                print("ü§ó skipping upload to HuggingFace Hub.")
        else:
            if internal['uploading']:
                internal['needs_upload'] = True
            else:
                internal['uploading'] = True
                internal['needs_upload'] = False
                fut = upload_ggufs_to_hf()
                fut.add_done_callback(lambda f: internal.__setitem__('uploading', False))
                internal['upload_futs'].append(fut)

if not args.skip_upload:
    for fut in internal['upload_futs']:
        fut.result()

    if internal['needs_upload']:
        upload_ggufs_to_hf().result()

print("üéâ done!")
